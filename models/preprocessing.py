"""
ì‹ í˜¸ ì²˜ë¦¬ ê¸°ë°˜ ì „ì²˜ë¦¬ í•„í„°
ë”¥ëŸ¬ë‹ ëª¨ë¸ ì „ì— ì ìš©í•˜ì—¬ ë§ˆì´í¬ ì¡ìŒì„ 1ì°¨ì ìœ¼ë¡œ ê°ì‡ 
"""

import torch
import torch.nn as nn
import numpy as np
from scipy import signal
from typing import Optional


class ProximityEffectCorrector(nn.Module):
    """
    ê·¼ì ‘íš¨ê³¼ ë³´ì • í•„í„°
    80Hz ì´í•˜ ì €ì£¼íŒŒë¥¼ ê°ì‡ ì‹œì¼œ ê·¼ì ‘íš¨ê³¼ë¥¼ ë³´ì •
    """
    
    def __init__(self, sample_rate: int = 16000, cutoff_freq: int = 80):
        super().__init__()
        self.sample_rate = sample_rate
        self.cutoff_freq = cutoff_freq
        
        # High-pass filter ê³„ìˆ˜ ê³„ì‚° (Butterworth)
        sos = signal.butter(4, cutoff_freq, 'hp', fs=sample_rate, output='sos')
        
        # SOS (Second-Order Sections) í˜•ì‹ì„ ì§ì ‘ ê³„ìˆ˜ë¡œ ë³€í™˜
        # PyTorchì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥
        self.register_buffer('sos', torch.FloatTensor(sos))
    
    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        """
        Args:
            audio: [batch, samples] ë˜ëŠ” [samples]
            
        Returns:
            í•„í„°ë§ëœ ì˜¤ë””ì˜¤
        """
        # NumPyë¡œ ë³€í™˜í•˜ì—¬ scipy í•„í„° ì ìš©
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)
            squeeze_output = True
        else:
            squeeze_output = False
        
        batch_size = audio.shape[0]
        output = []
        
        for i in range(batch_size):
            audio_np = audio[i].cpu().numpy()
            filtered = signal.sosfilt(self.sos.cpu().numpy(), audio_np)
            output.append(torch.FloatTensor(filtered))
        
        output = torch.stack(output).to(audio.device)
        
        if squeeze_output:
            output = output.squeeze(0)
        
        return output


class PopNoiseDetector(nn.Module):
    """
    íŒë…¸ì´ì¦ˆ ê°ì§€ ë° ì–µì œ
    ì§§ì€ ì‹œê°„ ë™ì•ˆì˜ ì—ë„ˆì§€ ê¸‰ì¦ì„ ê°ì§€í•˜ê³  soft clipping ì ìš©
    """
    
    def __init__(
        self, 
        sample_rate: int = 16000,
        window_size: int = 512,
        threshold: float = 3.0
    ):
        """
        Args:
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            window_size: ì—ë„ˆì§€ ê³„ì‚° ìœˆë„ìš° í¬ê¸°
            threshold: íŒ ê°ì§€ ì„ê³„ê°’ (í‰ê·  ì—ë„ˆì§€ì˜ ë°°ìˆ˜)
        """
        super().__init__()
        self.sample_rate = sample_rate
        self.window_size = window_size
        self.threshold = threshold
    
    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        """
        Args:
            audio: [batch, samples] ë˜ëŠ” [samples]
            
        Returns:
            íŒë…¸ì´ì¦ˆê°€ ì–µì œëœ ì˜¤ë””ì˜¤
        """
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)
            squeeze_output = True
        else:
            squeeze_output = False
        
        batch_size, num_samples = audio.shape
        output = audio.clone()
        
        for i in range(batch_size):
            # ì—ë„ˆì§€ ê³„ì‚° (sliding window)
            audio_squared = audio[i] ** 2
            energy = torch.nn.functional.avg_pool1d(
                audio_squared.unsqueeze(0).unsqueeze(0),
                kernel_size=self.window_size,
                stride=1,
                padding=self.window_size // 2
            ).squeeze()
            
            # í‰ê·  ì—ë„ˆì§€
            mean_energy = energy.mean()
            
            # íŒ ê°ì§€: ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” êµ¬ê°„
            pop_mask = energy > (mean_energy * self.threshold)
            
            # íŒ êµ¬ê°„ì— soft clipping ì ìš©
            if pop_mask.any():
                # ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
                pop_mask_full = torch.zeros(num_samples, dtype=torch.bool, device=audio.device)
                valid_length = min(len(pop_mask), num_samples)
                pop_mask_full[:valid_length] = pop_mask[:valid_length]
                
                # Soft clipping (tanh)
                output[i, pop_mask_full] = torch.tanh(output[i, pop_mask_full] * 2) * 0.5
        
        if squeeze_output:
            output = output.squeeze(0)
        
        return output


class ElectricalNoiseFilter(nn.Module):
    """
    ì „ê¸°ì  ì¡ìŒ í•„í„°
    50/60Hz í—˜(hum) ì œê±°ë¥¼ ìœ„í•œ notch filter
    """
    
    def __init__(
        self, 
        sample_rate: int = 16000,
        hum_freq: int = 60,
        quality_factor: float = 30.0
    ):
        """
        Args:
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            hum_freq: í—˜ ì£¼íŒŒìˆ˜ (50 or 60 Hz)
            quality_factor: ë…¸ì¹˜ í•„í„° Q íŒ©í„° (ë†’ì„ìˆ˜ë¡ ì¢ì€ ëŒ€ì—­)
        """
        super().__init__()
        self.sample_rate = sample_rate
        self.hum_freq = hum_freq
        
        # Notch filter ê³„ìˆ˜ ê³„ì‚° (ê¸°ë³¸ ì£¼íŒŒìˆ˜ + ê³ ì¡°íŒŒ)
        freqs_to_remove = [hum_freq, hum_freq * 2, hum_freq * 3]
        
        sos_list = []
        for freq in freqs_to_remove:
            if freq < sample_rate / 2:  # Nyquist ì£¼íŒŒìˆ˜ ì´í•˜ë§Œ
                sos = signal.iirnotch(freq, quality_factor, sample_rate)
                # sos í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                sos_cascade = signal.tf2sos(sos[0], sos[1])
                sos_list.append(sos_cascade)
        
        # ëª¨ë“  notch filterë¥¼ cascaded SOSë¡œ ê²°í•©
        if sos_list:
            combined_sos = np.vstack(sos_list)
            self.register_buffer('sos', torch.FloatTensor(combined_sos))
        else:
            self.register_buffer('sos', torch.FloatTensor([]))
    
    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        """
        Args:
            audio: [batch, samples] ë˜ëŠ” [samples]
            
        Returns:
            í—˜ì´ ì œê±°ëœ ì˜¤ë””ì˜¤
        """
        if len(self.sos) == 0:
            return audio
        
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)
            squeeze_output = True
        else:
            squeeze_output = False
        
        batch_size = audio.shape[0]
        output = []
        
        for i in range(batch_size):
            audio_np = audio[i].cpu().numpy()
            filtered = signal.sosfilt(self.sos.cpu().numpy(), audio_np)
            output.append(torch.FloatTensor(filtered))
        
        output = torch.stack(output).to(audio.device)
        
        if squeeze_output:
            output = output.squeeze(0)
        
        return output


class MicrophoneNoisePreprocessor(nn.Module):
    """
    ëª¨ë“  ë§ˆì´í¬ ì¡ìŒ ì „ì²˜ë¦¬ í•„í„°ë¥¼ í†µí•©í•œ ëª¨ë“ˆ
    """
    
    def __init__(
        self,
        sample_rate: int = 16000,
        apply_proximity_correction: bool = True,
        apply_pop_suppression: bool = True,
        apply_hum_removal: bool = True,
        hum_freq: int = 60
    ):
        super().__init__()
        
        self.apply_proximity_correction = apply_proximity_correction
        self.apply_pop_suppression = apply_pop_suppression
        self.apply_hum_removal = apply_hum_removal
        
        if apply_proximity_correction:
            self.proximity_corrector = ProximityEffectCorrector(sample_rate)
        
        if apply_pop_suppression:
            self.pop_detector = PopNoiseDetector(sample_rate)
        
        if apply_hum_removal:
            self.hum_filter = ElectricalNoiseFilter(sample_rate, hum_freq)
    
    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        """
        ëª¨ë“  ì „ì²˜ë¦¬ í•„í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©
        
        Args:
            audio: [batch, samples] ë˜ëŠ” [samples]
            
        Returns:
            ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤
        """
        # 1. ê·¼ì ‘íš¨ê³¼ ë³´ì •
        if self.apply_proximity_correction:
            audio = self.proximity_corrector(audio)
        
        # 2. ì „ê¸° ì¡ìŒ ì œê±°
        if self.apply_hum_removal:
            audio = self.hum_filter(audio)
        
        # 3. íŒë…¸ì´ì¦ˆ ì–µì œ (ë§ˆì§€ë§‰ì— ì ìš©)
        if self.apply_pop_suppression:
            audio = self.pop_detector(audio)
        
        return audio


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ›ï¸ ì „ì²˜ë¦¬ í•„í„° í…ŒìŠ¤íŠ¸...")
    
    # í…ŒìŠ¤íŠ¸ ì‹ í˜¸ ìƒì„± (1ì´ˆ, 16kHz)
    sr = 16000
    duration = 1.0
    t = np.linspace(0, duration, int(sr * duration))
    
    # ìŒì„± ì‹œë®¬ë ˆì´ì…˜ (ì—¬ëŸ¬ ì£¼íŒŒìˆ˜ í˜¼í•©)
    audio = 0.5 * np.sin(2 * np.pi * 200 * t)  # 200Hz
    audio += 0.3 * np.sin(2 * np.pi * 400 * t)  # 400Hz
    
    # 60Hz í—˜ ì¶”ê°€
    audio += 0.1 * np.sin(2 * np.pi * 60 * t)
    
    audio_tensor = torch.FloatTensor(audio)
    
    # ì „ì²˜ë¦¬ê¸° ìƒì„±
    preprocessor = MicrophoneNoisePreprocessor(sample_rate=sr, hum_freq=60)
    
    # í•„í„° ì ìš©
    filtered = preprocessor(audio_tensor)
    
    print(f"   ì…ë ¥ shape: {audio_tensor.shape}")
    print(f"   ì¶œë ¥ shape: {filtered.shape}")
    print(f"   ì…ë ¥ RMS: {audio_tensor.pow(2).mean().sqrt():.4f}")
    print(f"   ì¶œë ¥ RMS: {filtered.pow(2).mean().sqrt():.4f}")
    print("âœ… ì „ì²˜ë¦¬ í•„í„° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

