"""
U-Net ê¸°ë°˜ ìŒì„± í–¥ìƒ ëª¨ë¸ (ë² ì´ìŠ¤ë¼ì¸)

ì‹œê°„-ì£¼íŒŒìˆ˜(T-F) ë„ë©”ì¸ì—ì„œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì²˜ë¦¬í•˜ì—¬
ì¡ìŒì„ ì œê±°í•˜ëŠ” U-Net ì•„í‚¤í…ì²˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class ConvBlock(nn.Module):
    """U-Netì˜ ê¸°ë³¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡"""
    
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, 
                     stride=stride, padding=kernel_size//2),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size, 
                     stride=1, padding=kernel_size//2),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        return self.conv(x)


class DownBlock(nn.Module):
    """U-Netì˜ ì¸ì½”ë” ë¸”ë¡ (ë‹¤ìš´ìƒ˜í”Œë§)"""
    
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = ConvBlock(in_channels, out_channels)
        self.pool = nn.MaxPool2d(2)
    
    def forward(self, x):
        conv_out = self.conv(x)
        pool_out = self.pool(conv_out)
        return conv_out, pool_out


class UpBlock(nn.Module):
    """U-Netì˜ ë””ì½”ë” ë¸”ë¡ (ì—…ìƒ˜í”Œë§)"""
    
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, 
                                         kernel_size=2, stride=2)
        self.conv = ConvBlock(out_channels * 2, out_channels)
    
    def forward(self, x, skip):
        x = self.upconv(x)
        
        # Skip connectionê³¼ í¬ê¸° ë§ì¶”ê¸°
        if x.shape != skip.shape:
            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)
        
        x = torch.cat([x, skip], dim=1)
        x = self.conv(x)
        return x


class SpectrogramUNet(nn.Module):
    """
    ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ê¸°ë°˜ U-Net ìŒì„± í–¥ìƒ ëª¨ë¸
    
    ì…ë ¥: ì¡ìŒì´ ì„ì¸ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
    ì¶œë ¥: ê¹¨ë—í•œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë˜ëŠ” ë§ˆìŠ¤í¬
    """
    
    def __init__(
        self,
        n_fft: int = 512,
        hop_length: int = 256,
        n_channels: int = 32,
        output_mode: str = "mask"  # "mask" or "spectrogram"
    ):
        """
        Args:
            n_fft: FFT í¬ê¸°
            hop_length: STFT hop length
            n_channels: ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ì±„ë„ ìˆ˜
            output_mode: "mask"ë©´ ë§ˆìŠ¤í¬ ì˜ˆì¸¡, "spectrogram"ì´ë©´ ì§ì ‘ ì˜ˆì¸¡
        """
        super().__init__()
        
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.output_mode = output_mode
        
        # ì¸ì½”ë” (ë‹¤ìš´ìƒ˜í”Œë§)
        self.down1 = DownBlock(1, n_channels)          # -> 32
        self.down2 = DownBlock(n_channels, n_channels*2)     # -> 64
        self.down3 = DownBlock(n_channels*2, n_channels*4)   # -> 128
        self.down4 = DownBlock(n_channels*4, n_channels*8)   # -> 256
        
        # ë³´í‹€ë„¥
        self.bottleneck = ConvBlock(n_channels*8, n_channels*16)  # 512
        
        # ë””ì½”ë” (ì—…ìƒ˜í”Œë§)
        self.up1 = UpBlock(n_channels*16, n_channels*8)    # 256
        self.up2 = UpBlock(n_channels*8, n_channels*4)     # 128
        self.up3 = UpBlock(n_channels*4, n_channels*2)     # 64
        self.up4 = UpBlock(n_channels*2, n_channels)       # 32
        
        # ì¶œë ¥ ë ˆì´ì–´
        if output_mode == "mask":
            # ë§ˆìŠ¤í¬ ì˜ˆì¸¡ (0~1 ì‚¬ì´ ê°’)
            self.output = nn.Sequential(
                nn.Conv2d(n_channels, 1, kernel_size=1),
                nn.Sigmoid()
            )
        else:
            # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì§ì ‘ ì˜ˆì¸¡
            self.output = nn.Conv2d(n_channels, 1, kernel_size=1)
    
    def forward(self, noisy_audio: torch.Tensor) -> torch.Tensor:
        """
        Args:
            noisy_audio: [batch, samples] - ì¡ìŒì´ ì„ì¸ ì˜¤ë””ì˜¤
            
        Returns:
            enhanced_audio: [batch, samples] - í–¥ìƒëœ ì˜¤ë””ì˜¤
        """
        # 1. STFT ë³€í™˜
        stft = torch.stft(
            noisy_audio,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            return_complex=True,
            window=torch.hann_window(self.n_fft).to(noisy_audio.device)
        )
        
        # Magnitudeì™€ Phase ë¶„ë¦¬
        magnitude = torch.abs(stft)  # [batch, freq, time]
        phase = torch.angle(stft)
        
        # 2. U-Net ì²˜ë¦¬ (magnitudeë§Œ)
        x = magnitude.unsqueeze(1)  # [batch, 1, freq, time]
        
        # ì¸ì½”ë”
        skip1, x = self.down1(x)
        skip2, x = self.down2(x)
        skip3, x = self.down3(x)
        skip4, x = self.down4(x)
        
        # ë³´í‹€ë„¥
        x = self.bottleneck(x)
        
        # ë””ì½”ë” (skip connections)
        x = self.up1(x, skip4)
        x = self.up2(x, skip3)
        x = self.up3(x, skip2)
        x = self.up4(x, skip1)
        
        # ì¶œë ¥
        output = self.output(x).squeeze(1)  # [batch, freq, time]
        
        # 3. ë§ˆìŠ¤í¬ ì ìš© ë˜ëŠ” ì§ì ‘ ì‚¬ìš©
        if self.output_mode == "mask":
            # ì˜ˆì¸¡ëœ ë§ˆìŠ¤í¬ë¥¼ magnitudeì— ê³±í•¨
            enhanced_magnitude = magnitude * output
        else:
            # ì§ì ‘ ì˜ˆì¸¡ëœ magnitude ì‚¬ìš©
            enhanced_magnitude = output
        
        # í¬ê¸° ë§ì¶”ê¸°
        if enhanced_magnitude.shape != magnitude.shape:
            enhanced_magnitude = F.interpolate(
                enhanced_magnitude.unsqueeze(1), 
                size=magnitude.shape[1:], 
                mode='bilinear',
                align_corners=False
            ).squeeze(1)
        
        # 4. ì›ë³¸ ìœ„ìƒ ì‚¬ìš©í•˜ì—¬ iSTFT
        enhanced_stft = enhanced_magnitude * torch.exp(1j * phase)
        
        # iSTFT
        enhanced_audio = torch.istft(
            enhanced_stft,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            window=torch.hann_window(self.n_fft).to(noisy_audio.device),
            length=noisy_audio.shape[-1]
        )
        
        return enhanced_audio


class WaveformUNet(nn.Module):
    """
    ì‹œê°„ ë„ë©”ì¸ ì§ì ‘ ì²˜ë¦¬ U-Net (1D Convolution)
    
    STFT ì—†ì´ waveformì„ ì§ì ‘ ì²˜ë¦¬ - End-to-End
    """
    
    def __init__(self, n_channels: int = 32):
        super().__init__()
        
        # ì¸ì½”ë” (1D Conv)
        self.enc1 = nn.Sequential(
            nn.Conv1d(1, n_channels, kernel_size=15, stride=1, padding=7),
            nn.BatchNorm1d(n_channels),
            nn.LeakyReLU(0.2)
        )
        self.enc2 = nn.Sequential(
            nn.Conv1d(n_channels, n_channels*2, kernel_size=15, stride=2, padding=7),
            nn.BatchNorm1d(n_channels*2),
            nn.LeakyReLU(0.2)
        )
        self.enc3 = nn.Sequential(
            nn.Conv1d(n_channels*2, n_channels*4, kernel_size=15, stride=2, padding=7),
            nn.BatchNorm1d(n_channels*4),
            nn.LeakyReLU(0.2)
        )
        self.enc4 = nn.Sequential(
            nn.Conv1d(n_channels*4, n_channels*8, kernel_size=15, stride=2, padding=7),
            nn.BatchNorm1d(n_channels*8),
            nn.LeakyReLU(0.2)
        )
        
        # ë³´í‹€ë„¥
        self.bottleneck = nn.Sequential(
            nn.Conv1d(n_channels*8, n_channels*16, kernel_size=15, stride=2, padding=7),
            nn.BatchNorm1d(n_channels*16),
            nn.LeakyReLU(0.2)
        )
        
        # ë””ì½”ë” (Transposed Conv)
        self.dec1 = nn.Sequential(
            nn.ConvTranspose1d(n_channels*16, n_channels*8, kernel_size=15, 
                              stride=2, padding=7, output_padding=0),
            nn.BatchNorm1d(n_channels*8),
            nn.ReLU()
        )
        self.dec2 = nn.Sequential(
            nn.ConvTranspose1d(n_channels*16, n_channels*4, kernel_size=15,
                              stride=2, padding=7, output_padding=0),
            nn.BatchNorm1d(n_channels*4),
            nn.ReLU()
        )
        self.dec3 = nn.Sequential(
            nn.ConvTranspose1d(n_channels*8, n_channels*2, kernel_size=15,
                              stride=2, padding=7, output_padding=0),
            nn.BatchNorm1d(n_channels*2),
            nn.ReLU()
        )
        self.dec4 = nn.Sequential(
            nn.ConvTranspose1d(n_channels*4, n_channels, kernel_size=15,
                              stride=2, padding=7, output_padding=0),
            nn.BatchNorm1d(n_channels),
            nn.ReLU()
        )
        
        # ì¶œë ¥
        self.output = nn.Sequential(
            nn.Conv1d(n_channels*2, 1, kernel_size=15, padding=7),
            nn.Tanh()
        )
    
    def forward(self, noisy_audio: torch.Tensor) -> torch.Tensor:
        """
        Args:
            noisy_audio: [batch, samples]
            
        Returns:
            enhanced_audio: [batch, samples]
        """
        x = noisy_audio.unsqueeze(1)  # [batch, 1, samples]
        
        # ì¸ì½”ë”
        e1 = self.enc1(x)
        e2 = self.enc2(e1)
        e3 = self.enc3(e2)
        e4 = self.enc4(e3)
        
        # ë³´í‹€ë„¥
        b = self.bottleneck(e4)
        
        # ë””ì½”ë” (skip connections)
        d1 = self.dec1(b)
        # í¬ê¸° ë§ì¶”ê¸°
        if d1.shape[2] != e4.shape[2]:
            d1 = F.interpolate(d1, size=e4.shape[2], mode='linear', align_corners=False)
        d1 = torch.cat([d1, e4], dim=1)
        
        d2 = self.dec2(d1)
        if d2.shape[2] != e3.shape[2]:
            d2 = F.interpolate(d2, size=e3.shape[2], mode='linear', align_corners=False)
        d2 = torch.cat([d2, e3], dim=1)
        
        d3 = self.dec3(d2)
        if d3.shape[2] != e2.shape[2]:
            d3 = F.interpolate(d3, size=e2.shape[2], mode='linear', align_corners=False)
        d3 = torch.cat([d3, e2], dim=1)
        
        d4 = self.dec4(d3)
        if d4.shape[2] != e1.shape[2]:
            d4 = F.interpolate(d4, size=e1.shape[2], mode='linear', align_corners=False)
        d4 = torch.cat([d4, e1], dim=1)
        
        # ì¶œë ¥
        output = self.output(d4).squeeze(1)  # [batch, samples]
        
        # ì›ë³¸ ê¸¸ì´ ë§ì¶”ê¸°
        if output.shape[-1] != noisy_audio.shape[-1]:
            output = F.interpolate(
                output.unsqueeze(1), 
                size=noisy_audio.shape[-1],
                mode='linear',
                align_corners=False
            ).squeeze(1)
        
        return output


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ§ª U-Net ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 4
    sample_rate = 16000
    duration = 2.0
    num_samples = int(sample_rate * duration)
    
    noisy_audio = torch.randn(batch_size, num_samples)
    
    # 1. SpectrogramUNet í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ SpectrogramUNet (T-F Domain)")
    model_spec = SpectrogramUNet(n_fft=512, hop_length=256, output_mode="mask")
    
    with torch.no_grad():
        enhanced = model_spec(noisy_audio)
    
    print(f"   ì…ë ¥ shape: {noisy_audio.shape}")
    print(f"   ì¶œë ¥ shape: {enhanced.shape}")
    print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model_spec.parameters()) / 1e6:.2f}M")
    
    # 2. WaveformUNet í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ WaveformUNet (Time Domain)")
    model_wave = WaveformUNet(n_channels=32)
    
    with torch.no_grad():
        enhanced = model_wave(noisy_audio)
    
    print(f"   ì…ë ¥ shape: {noisy_audio.shape}")
    print(f"   ì¶œë ¥ shape: {enhanced.shape}")
    print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model_wave.parameters()) / 1e6:.2f}M")
    
    print("\nâœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

